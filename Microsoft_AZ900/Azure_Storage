
Azure Storage

Azure Storage is a Microsoft's cloud storage solution for modern enterprises and applications to store data

Advantages of Azure Storage

Scalability: It is easily scalable. You can start with a small size blob and scale to infinite size depending on the demand without affecting the production.
Durability: Data in Azure storage is very durable with multiple levels of redundancy in which you have just to mention the type of redundancy and Azure does the rest of the work for you.
Availability: Replication option enables you to store copies of your data in different availability zones, and in the event of zone or node destruction, the data is still available.
Security: All data in the Storage is encrypted; both in rest and transit. Also, with key vault, only you have control over encryption keys making your data more secure.
Accessibility: The data in storage can be easily accessed through the available SDKs in a wide variety of languages. Azure Storage supports scripting in Azure PowerShell or Azure CLI.
Management: Management is easy because Azure takes care of any critical problems and maintenance of the servers.

Intro to Unstructured Storage

Azure unstructured data consists of disk storage, file store, sync, and blob storage

Storage Account: To use any of the Azure Storage services, you need to create a storage account within the Azure account. Resources in this account can be accessed by any language that can make HTTP/HTTPS request.
Azure Blob: Binary Large Object (BLOB) is a collection of binary data that may be a file, video, text, image, etc. Azure Blob storage is Microsoft's object storage solution for the cloud.
Container: A container provides a grouping of a set of blobs. All blobs must be in a container. A Storage account can hold an unlimited number of containers. Likeways, a container can store an unlimited number of blobs.

VM Disks: Virtual Machines in Azure use disks as a place to store, which is either Hard Disk Drives (HDD) or Solid-state Drives (SSD), based on the storage account type.
Standard Storage: Standard Storage account is a type of storage account backed by magnetic drives (HDD) that provide low cost per GB and can be used for all kinds of data (blobs, files, tables).
Premium Storage: Premium storage has high bandwidth with extremely low latency. It offers less than 1 ms read latency (cache), and uses SSD. Premium Storage supports only Locally Redundant Storage (LRS) Replication


Storage Explorer: It is a useful GUI tool to inspect and alter data in Azure Storage. It can be used to upload, download, and manage blobs, files, queues, and tables from any platform, anywhere. It is provided by Microsoft.
Block Blobs: In block blobs, the blobs are comprised of blocks that are identified by unique block ID. The maximum size of blocks is restricted to 100MB, and each block blob can have 50,000 blocks which are ideal for storing text or binary files, such as documents and media files.
Page Blobs: Page blobs are the collection of 512-byte pages optimized for random read and write. These are used for storing VHD files of Azure VM as OS and Data disks. The maximum size of page blobs is 8TB.
Append Blob: This type is used to append blobs that are used for logging scenarios. The maximum size of append blob is 195 GB.

VM Machine Storage

A virtual machine uses three types of disks, they are:

Operating System Disk: It is created along with the virtual machine by Azure. It is generally mounted as C drive, which is 2 GB in size.
Temporary Disk: This disk holds temporary contents, although, you are not suggested to store anything there. If the VM is redeployed for any reason, the drive is wiped out and starts fresh. The primary function of that temporary disk is simply for the page file.
Data Disk stores all the data required to be worked upon. This disk is huge and has high performance.

Data Disks are where the working data and all the application data reside. The size of VM decides the number of data disks that can be attached to a VM. Data disks are stored in a BLOB in an Azure storage account.

Data disks could be of type Standard or Premium, both of which may be again managed or unmanaged.
Standard storage uses HDDs with a maximum IOPS (I/O Operations per second) of 50000 and the number of disks up to 100 (each disk is of 4 GB).
Premium storage has an unlimited IOPS and has 35 TB of storage and 10 TB of snapshots.

With Managed disks, Azure does all the management such as storage account creation, replication and also makes sure disks are kept secure.
With unmanaged disks, you will have the additional responsibility of all these administering tasks.
Pricing is per storage: For Unmanaged model, it uses storage account pricing. For the Managed model, it is simply based on the size of the disk.

Azure CLI 2.0 is known as a command line tool used for accessing the resources in Azure. The CLI is designed to make scripting easy, flexible query data, support long-running operations as non-blocking processes, and more.

Let's check some basic commands:

Creating Storage Account:
az storage account create --name accName --resource-group ResGrpName --sku standard_LRS --location useast.
Creating a Container:
az storage container create --name mycontainer --public-access blob/container
Use "blob" to allow public read access for blobs. Use "container" to allow public read and list access to the entire container.

Uploading a blob:
az storage blob upload --container mycontainer --name blobname --file filepath.
Downloading a blob:
az storage blob download --resource-group resgrpName --name blobName --file savingPath.
Deleting a Storage Account:
az storage account delete --name accname.

Uploading a blob:
az storage blob upload --container mycontainer --name blobname --file filepath.
Downloading a blob:
az storage blob download --resource-group resgrpName --name blobName --file savingPath.
Deleting a Storage Account:
az storage account delete --name accname.

Lab 01 :  Create an unmanaged VM and attach data disks to it

assign the resource group as default
az configure --defaults group=$resource

1)Create a storage account with name as per your wish and create a container with the name vmblob
az storage account create -n myfrescoaccount -g $resource -l westus --sku Standard_LRS

key=$(az storage account keys list -g $resource -n myfrescoaccount --query [0].value -o tsv)

az storage container create \
    --account-name myfrescoaccount \
    --name vmblob \
    --account-key $key \
    --auth-mode key

2) Create an unmanaged virtual machine in the resource group created by the katacoda account, with an admin username and admin password, and the following data:
image = Win2012R2Datacenter  , size = Standard_DS1_V2

az vm create -n unmanagedvm -g $resource --image Win2012R2Datacenter --size Standard_DS1_V2 \
--use-unmanaged-disk --admin-username frescouser --admin-password frescopwd@12345

3) Attach a new unmanaged data disk of 512 GB to the VM.

az vm unmanaged-disk attach -g $resource --vm-name unmanagedvm --new --size-gb 512

4) Convert the existing unmanaged VM to a managed VM, and restart the VM.

# deallocate the VM
az vm deallocate --resource-group $resource --name unmanagedvm

# Convert VM to Managed Disks
az vm convert --resource-group $resource --name unmanagedvm

# Start the VM back up again
az vm start --resource-group $resource --name unmanagedvm



Lab 02 : Azure Storage : Upload a Blob to the Storage Account

Create a storage account with blob encryption and only https traffic enabled.
az storage account create -n myfrescostorageact -g $resource --encryption-services blob --https-only true

key=$(az storage account keys list -g $resource -n myfrescostorageact --query [0].value -o tsv)

Create a blob container in the account.
az storage container create \
    --account-name myfrescostorageact \
    --name blobcontainer \
    --account-key $key \
    --auth-mode key

Upload a blob into the container of the storage account
az storage blob upload \
    --account-name myfrescostorageact \
    --container-name blobcontainer \
    --name index.php \
    --file index.php \
    --account-key $key \
    --auth-mode key


Azure File Storage

Azure File Storage provides shared storage for applications using Server Message Block (SMB) 3.0 protocol. Azure file shares can be mounted concurrently by cloud or on-premises deployments.
Applications that run in Azure cloud services or virtual machines can mount a File storage share to access file data, just as a desktop application. The Azure File shares can be cached on a Windows server using File Sync for faster access.
Azure Files also support snapshots of file shares. Share snapshots capture the shared state at that point in time which provides a backup during accidental deletions, data corruption, general backup, etc

Typical uses of File Storage are:

Migrating on-premises applications that depend on file shares to run on Azure cloud services or virtual machines.
Storing shared application settings, e.g., in configuration files.
Storing diagnostic data like crash dumps, metrics, and logs in a shared location.
Storing utilities and tools required for administering or developing Azure cloud services or virtual machines.

Azure File Sync

Storage Space is an administrative layer on top of storage pools that are formed by combining some physical hard disks in Windows servers.

From a user perspective, the storage space is represented as disks and volumes. You can access the volume from your operating system by using a drive letter.

Storage spaces are used for the following reasons:

Continuous Availability - Storage pools can be clustered across multiple nodes in the failover cluster. If a node fails, the storage will be seamlessly made available by a different node.
Multi-tenancy - Storage spaces hosting scenarios that do not require tenant storage isolation will offer multi-tenancy. It follows the familiar Windows security model and hence can be fully integrated with Active Directory Domain Services.
Simplicity - Storage Spaces are easily managed with Server Manager or Windows PowerShell. It also provides notifications when the amount of available capacity in a storage pool hits a configurable threshold.
Data Integrity - Storage Spaces includes background scrubbing and intelligent error correction to allow continuous service availability despite storage component failures.

Structured Storage

Azure offers the following Structured Storage Services to store structured and semi-structured data,

Azure Table Storage
Queue Storage
Document or Cosmos DB
Azure SQL DB
Azure’s Structured Storage is a highly available, resilient and affordable platform (IaaS and PaaS) for storing the business critical data\applications.

Azure Table Storage
Let's learn one of the crucial and most used structured storage service "Table".

Azure Table storage service is used to store structured NoSQL data in the cloud. Tables form a flexible store for flexible datasets, datasets that do not require complex joins, foreign keys.

Features:

NoSQL datastore accepts authenticated calls from inside and outside the Azure cloud.
NoSQL provides a key/attribute store with a schemaless design.
As it is schemaless, it's easy to adapt your data as the needs of your application evolves.
It is a cost-effective solution.
Access to Table storage data is fast with the OData protocol.
Table storage can be accessed directly as
 http://<storage account>.table.core.windows.net/<table>

Azure tables are ideal to store and query huge set of structured, non-relational data.

Tables storage will scale as demand increases.

Common Scenarios are:

Used in Storing huge amount of structured data that serves` web applications
Storing datasets that doesn't need complex joins, foreign keys, and stored procedures and can be customized for faster access
Quickly querying data using a clustered index

Table Storage component

Storage Account is a globally unique entity that is the basis for authentication and it is the parent namespace for the Table service.
Table is a collection of entities that don't have a forced schema on entities, in which a single table can contain many entities that have a different set of properties.
Entity is a set of properties, similar to that of rows in a database which can be up to 1 MB in size.
Properties - They are name-value pairs; each entity can consist of up to 255 properties to store data.

An entity in a table can have up to 255 properties, of which three are system properties:

PartitionKey - Entities with the same partition key can be queried more quickly, and inserted/updated in atomic operations.

RowKey - An entity's row key is its unique identifier within a partition.
Timestamp - The server manages the value of Timestamp, which cannot be modified.
The total size of all data in an entity's properties cannot be more than 1 MB.
The PartitionKey and RowKey must uniquely identify every entity in a table.

Azure Queues

Azure Queue Storage is a storage service used for storing large numbers of messages that can be accessed from anywhere via authenticated calls.

The size of a single message can be 64kb.
The time for which the messages stay in the queue is 7 Days.
Queues can extend up to the total capacity limit of Storage account.

Common use cases:

Creating a backlog of work to process asynchronously.
Passing messages from an Azure web role to an Azure worker role.
Queue storage can be accessed as http://<account>.queue.core.windows.net/<QueueName>

Azure CLI 2.0 Commands
To create a Queue:
az storage queue create --name Qname --account-name myAcc --account-key Mykey.
To delete the existing Queue:
az storage queue delete --name Qname --account-name Myacc --account-key Acckey 
To put a message in a Queue:
az storage message put --content mymessage --queue-name qname 
To get a message in Queue:
az storage message get --queue-name Qname
To retrieve one or more messages from the front of the queue:
az storage message peek --queue-name Qname.

To create a table:
az storage table create --name tablename --account-name Accname --access-key Acckey
To delete a table:
az storage table delete --name tablename --account-name Accname --access-key Acckey
To insert a storage entity into a table:
az storage entity insert --connection-string $connectionString --entity PartitionKey=AAA RowKey=BBB Content=ASDF2 --table-name tableName
To update an existing entity:
az storage entity merge --entity PartitionKey=AAA RowKey=BBB Content=ASDF2 --table-name tableName --account-key accKey --account-name accountName --connection-string $connectionString
To delete an entity:
az storage entity delete --partition-key KEY --row-key RKEY --table-name TName.

Lab 03 : CRUD operations on Azure Table Storage

1) Create a storage account with only https traffic enabled

az storage account create -n myfrescostorageact -g $resource --https-only true

key=$(az storage account keys list -g $resource -n myfrescostorageact --query [0].value -o tsv)

2) Create a table named Frescotab in the storage account.

az storage table create --name Frescotab --account-name myfrescostorageact 

3) Operations on Entities

Add two entities apart from PartitionKey and RowKey. For example, customers and contacts.

az storage entity insert --entity PartitionKey=AAA RowKey=BBG customers=XYZ contacts=999 --table-name Frescotab \
    --account-name myfrescostorageact --account-key $key

Add five or more lines of datasets to the table.

different RowKey inserted.

Update one or more entities by changing the name of customer.

az storage entity merge --entity PartitionKey=AAA RowKey=BBG customers=XXX contacts=999 \
                        --table-name Frescotab \
                        --account-key $key \
                        --account-name myfrescostorageact

Check the updated entities.

az storage entity query --table-name Frescotab --account-key $key --account-name myfrescostorageact

Delete a single line of a dataset.

az storage entity delete --partition-key AAA \
                         --row-key BBG \
                         --table-name Frescotab \
                         --account-key $key \
                         --account-name myfrescostorageact

Lab 03 : Working with Queues


What are Azure SQL Databases?
Azure SQL Database is a relational database that is offered as-a-service from Azure offering predictable performance and it is highly scalable, and each database is isolated.
Also, Azure SQL databases also provide monitoring and alerting as failover options.
With active geo-replication, the data is backed up in three copies in same or different locations. These secondaries can be used for read-only access.
After failover, the new primary has a different connection endpoint.

Database Tires 

Basic - for databases that have a single active operation, or for dev and test deployments.
Standard - for low to medium input/output databases and supports concurrent queries, as well as Azure web applications.
Premium - The Premium tier is designed for high transaction volumes and input/outputs. It supports multiple concurrent users, and Microsoft recommends that you use this database for mission-critical databases.
PremiumRS - This tier is recommended for input/output intensive workloads when high availability is not required. But you can use it for high-performance test and dev workloads.

SQL Database renders dynamically scalable performance in two diverse purchasing models - a vCore-based purchasing model and a DTU-based purchasing model.

vCore based model - In this model, you can independently choose the computing and storage resources. It also lets you use the Azure Hybrid Benefit for SQL Server to gain on your savings.
Database Transaction Units (DTU) based model - This model is based on a combined measure of computing, storage, and IO resources. Performance levels are expressed in terms of DTUs for single databases and elastic DTUs (eDTUs) for elastic pools.

Logical servers in Azure SQL offers both purchasing models: a DTU-based purchasing model and a vCore-based purchasing model.
Managed Instances in Azure SQL Database offer only the vCore-based purchasing model.

DTU and eDTU
The performance of SQL Database is based on DTU.

According to Microsoft, a DTU is a unit of measure of the resources that are guaranteed to be available to a single Azure SQL Database at a specific performance level within a single database tier. A DTU combines CPU, memory, data I/O, and transaction I/O.
Databases can be placed into an elastic pool on a SQL Database server that shares a pool of resources among those databases. The shared pool of resources are measured by elastic Database transition units (eDTU).

The advantages of an elastic pool are:

They are scaled automatically.
They provide predictable costs.
They are widely used for varying and unpredictable usage patterns

Determining DTUs for Workloads
If you are planning to move your on-premises Databases to Azure SQL Database, Azure provides a DTU Calculator to find the approximate DTUs required for setting up the database.

For an existing Azure Database, you can use SQL Data query performance insight. This Query performance insight provides a deeper insight into the resource consumption (DTU's) of Database, the top queries by CPU/Duration/Execution count, which can potentially be tuned for improved performance.

Azure SQL Data Sync
SQL Data Sync is a service developed on Azure SQL Database that allows you to synchronize the selected data bi-directionally across various SQL Server and SQL databases instances. Let's look into the following video to know more about it.

Azure SQL Database CLI 2.0 Commands
To create a MySQL Database with given charset and collation rules
az mysql db create --resource-group testgroup --server-name testsvr --name testdb --charset {valid_charset} --collation {valid_collation}
To delete a DB:
az mysql db delete -g testgroup -s testsvr -n testdb
List all DB in a server:
az mysql db list -g testgroup -s testsvr
To show an existing Db:
az mysql db show -g testgroup -s testsvr -n testdb

Azure Content Delivery Network (CDN)

The Azure Content Delivery Network (CDN) caches static web content at strategically placed locations from the Origin and provide maximum throughput for delivering content to the end users.

Benefits of CDN
Improved performance and user experience for end users, particularly in applications in which multiple round-trips are required to load content.
Large scaling and better handling of high instantaneous loads.
It distributes users requests and serves content directly from edge servers reducing traffic sent to the origin.

CDN Workflow

A user (John) requests a file (file1) using a URL, <endpointname>.azureedge.net.
DNS routes the request to the POP location, which is geographically closest to the user.
Now, consider that the edge servers does not have the requested file in their cache memory, then it requests the needed file from the origin.
Then the origin server returns the required file to the edge server.
Now the edge server caches the file in the memory and returns to the actual requester (John).
Then the file remains cached until the Time To Live (TTL) expires. The default TTL for the cache is 7 days.
Other users can also access the file from that same POP.
Here, origin can be anything like Azure's - Web App, Cloud Service, Storage account, or any publicly accessible web server.

Scenarios of CDN
Common Scenarios where CDN can be used:
Media Services to deliver video worldwide. The video is usually large and requires a lot of bandwidth.
Scenarios where CDN may be less useful:

For the content with a low hit rate setting.
For private data, like supply chain ecosystems or large enterprises.

CDN - Planning Factors

Performance – When considering performance, be sure this applies to all intended destinations.
Location – Select the ones that are more reliable and enhances the performance and viewing experience.
Scalability – After implementation, it should be easily expandable when it is needed.
Flexibility – Is the vendor service compatible with every form of technology your end users will potentially utilize? This is likely to include PCs, Mac computers, tablets, and smartphones.
Support – After signing up, will you receive the level of support you require with a continuous, reliable service?
Cost-effective – The available services may vary greatly in cost. Make sure that you are not signing up for a long-term service that cannot truly accommodate your needs at a cost-effective price.

Azure CDN CLI 2.0 Commands
To create a new CDN profile:
az cdn profile create --name testcdn --resource-group testgrp --sku premium_verizon
To create an endpoint with the origin as storage:
az cdn endpoint create --name testcdn --origin storage.blob.core.windows.net --profile-name profile --no-http
To delete an endpoint:
az cdn endpoint delete -g group -n endpoint --profile-name profile-name
To preload content for an endpoint:
az cdn endpoint load -g group -n endpoint --profile-name profile-name --content-paths '/scripts/app.js' '/styles/main.css'
content to be preloaded are to be mentioned in the above command.
To purge the preloaded content:
az cdn endpoint purge -g group -n endpoint --profile-name profile-name --content-paths '/scripts/app.js' '/styles/*'




